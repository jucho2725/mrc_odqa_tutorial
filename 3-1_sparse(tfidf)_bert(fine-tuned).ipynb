{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF retrieval + bert(fine-tuned)\n",
    "\n",
    "작성일자: 210118\\\n",
    "작성자: 조진욱\\\n",
    "목표: retrieval 모델과 우리가 학습시킨 bert 모델을 가지고 open domain qa 형식으로 만들어보자\\\n",
    "순서: \n",
    "1. \n",
    "query 가 나열되어있는 json 형태 파일 'dev_qa.json' 파일이 들어오면\n",
    "retrieval 모델이 query에 맞는 context 하나를 찾아 (c, q, a) pair 를 만들어줌. \n",
    "이를 dev_cqa.json 로 저장함.\n",
    "2. \n",
    "bert 모델은 dev_cqa.json 을 불러와 answer에 대한 답을 냄.\n",
    "그 뒤 squad_evaluate 함수를 통해 점수 확인 \n",
    "\n",
    "\n",
    "비고:\n",
    "1. load 함수에서 json 형태로 불러오도록 되어있어 필요없지만 retrieval 의 결과를 json 으로 저장하는 과정을 거침. \n",
    "2. 1-2 에서 학습한 bert 모델을 그대로 가져다씀 from_pretrained(cfg.output_dir)\n",
    "3. 그러나 1-2 best metric 보다 성능이 안나와야 정상. 왜냐하면 retrieval 과정에서 잘못된 context 들이 선택되었을 것이기 때문."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 retrieval 모델 불러와서 각 query 에 대한 document 찾기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg\n",
    "from retrieval import SparseRetrieval\n",
    "from utils import save_json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'dev'\n",
    "\n",
    "# context_file = f'{mode}_context.json'\n",
    "# qa_file =  f'{mode}_qa.json'\n",
    "# ret = SparseRetrieval(mode, data_path=cfg.squad_dir)\n",
    "\n",
    "# # knowledge base에 있는 articles(context) 들의 정보를 임베딩해둠\n",
    "# tfidfv, context_embeddings = ret.make_embedding(context_file)\n",
    "\n",
    "# cqa_df = ret.retrieve(tfidfv, context_embeddings, qa_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>que_id</th>\n",
       "      <th>answers</th>\n",
       "      <th>context</th>\n",
       "      <th>context_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is the creator of American Idol?</td>\n",
       "      <td>56d21eb1e7d4791d00902667</td>\n",
       "      <td>{'answer_start': 67, 'text': 'Simon Fuller'}</td>\n",
       "      <td>['American Idol is broadcast to over 100 natio...</td>\n",
       "      <td>['context124']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What company produces American idol?</td>\n",
       "      <td>56d21eb1e7d4791d00902668</td>\n",
       "      <td>{'answer_start': 96, 'text': '19 Entertainment'}</td>\n",
       "      <td>['American Idol is broadcast to over 100 natio...</td>\n",
       "      <td>['context124']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What year did American Idol begin airing?</td>\n",
       "      <td>56d21eb1e7d4791d00902669</td>\n",
       "      <td>{'answer_start': 201, 'text': '2002'}</td>\n",
       "      <td>['American Idol is broadcast to over 100 natio...</td>\n",
       "      <td>['context124']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What British show is American Idols format bas...</td>\n",
       "      <td>56d21eb1e7d4791d0090266a</td>\n",
       "      <td>{'answer_start': 270, 'text': 'Pop Idol'}</td>\n",
       "      <td>['American Idol is an American singing competi...</td>\n",
       "      <td>['context1']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What television network does American Idol air...</td>\n",
       "      <td>56d21eb1e7d4791d0090266b</td>\n",
       "      <td>{'answer_start': 185, 'text': 'Fox'}</td>\n",
       "      <td>['In Latin America, the show is broadcast and ...</td>\n",
       "      <td>['context125']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0               Who is the creator of American Idol?   \n",
       "1               What company produces American idol?   \n",
       "2          What year did American Idol begin airing?   \n",
       "3  What British show is American Idols format bas...   \n",
       "4  What television network does American Idol air...   \n",
       "\n",
       "                     que_id                                           answers  \\\n",
       "0  56d21eb1e7d4791d00902667      {'answer_start': 67, 'text': 'Simon Fuller'}   \n",
       "1  56d21eb1e7d4791d00902668  {'answer_start': 96, 'text': '19 Entertainment'}   \n",
       "2  56d21eb1e7d4791d00902669             {'answer_start': 201, 'text': '2002'}   \n",
       "3  56d21eb1e7d4791d0090266a         {'answer_start': 270, 'text': 'Pop Idol'}   \n",
       "4  56d21eb1e7d4791d0090266b              {'answer_start': 185, 'text': 'Fox'}   \n",
       "\n",
       "                                             context      context_id  \n",
       "0  ['American Idol is broadcast to over 100 natio...  ['context124']  \n",
       "1  ['American Idol is broadcast to over 100 natio...  ['context124']  \n",
       "2  ['American Idol is broadcast to over 100 natio...  ['context124']  \n",
       "3  ['American Idol is an American singing competi...    ['context1']  \n",
       "4  ['In Latin America, the show is broadcast and ...  ['context125']  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res_path = os.path.join(cfg.sparse_dir, f\"retrieved_{mode}_sparse.csv\")\n",
    "# cqa_df.to_csv(res_path, sep='\\t', index=False)\n",
    "# import pandas as pd\n",
    "# cqa_df = pd.read_csv(res_path, sep='\\t', index_col=False)\n",
    "# cqa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filename = f\"retrieved_{mode}_sparse.json\"\n",
    "# save_json(cqa_df, result_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 bert 모델이 만들어진 json 파일을 다시 불러와 MRC 진행\n",
    "\n",
    "torch Dataset의 형식으로 변환, mrc 형식으로 각 인스턴스에 대한 start end position 을 구한뒤 그에 대한 평가 진행 EM, F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    BertForQuestionAnswering,\n",
    "    BertTokenizer,\n",
    ")\n",
    "# from transformers.data.metrics.squad_metrics import (\n",
    "# from squad_metrics import(\n",
    "    compute_predictions_logits,\n",
    "    squad_evaluate,\n",
    ")\n",
    "\n",
    "from transformers.data.processors.squad import SquadResult, SquadProcessor, squad_convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg\n",
    "from utils import load_and_cache_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT + 마지막 cls 추가 레이어 존재함\n",
    "# 이미 학습된 모델이므로  Some weights of the model checkpoint at bert-large-cased were not used 와 같은 에러 발생하면 안됨\n",
    "model = BertForQuestionAnswering.from_pretrained(cfg.output_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(cfg.tokenizer_name)\n",
    "model = model.to(cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features from dataset file at %s ./data/squad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "convert squad examples to features: 100%|██████████| 4639/4639 [00:08<00:00, 565.17it/s]\n",
      "add example index and unique id: 100%|██████████| 4639/4639 [00:00<00:00, 1267086.24it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset, examples, features = load_and_cache_examples(cfg, tokenizer, mode_or_filename=result_filename, output_examples=True)\n",
    "eval_sampler = SequentialSampler(dataset)\n",
    "eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=cfg.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer):\n",
    "    print(\"***** Running evaluation *****\")\n",
    "    print(\"  Num examples = \", len(dataset))\n",
    "    print(\"  Batch size = \", cfg.eval_batch_size)\n",
    "    all_results = []\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(cfg.device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "            }\n",
    "\n",
    "            feature_indices = batch[3]\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        for i, feature_index in enumerate(feature_indices):\n",
    "            eval_feature = features[feature_index.item()]\n",
    "            unique_id = int(eval_feature.unique_id)\n",
    "\n",
    "            start_logits = outputs.start_logits[i]\n",
    "            end_logits = outputs.end_logits[i]\n",
    "            result = SquadResult(unique_id, start_logits, end_logits)\n",
    "\n",
    "            all_results.append(result)\n",
    "            \n",
    "    predictions = compute_predictions_logits(examples,\n",
    "                                            features,\n",
    "                                            all_results,\n",
    "                                            cfg.n_best_size,\n",
    "                                            cfg.max_answer_length,\n",
    "                                            False, # do_lower_case\n",
    "                                            None,  \n",
    "                                            None,\n",
    "#                                             None,  # \n",
    "                                            cfg.verbose_logging,\n",
    "#                                             False, #\n",
    "                                            cfg.null_score_diff_threshold,\n",
    "                                            tokenizer,)\n",
    "    \n",
    "    results = squad_evaluate(examples, predictions)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 3/583 [00:00<00:21, 27.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "  Num examples =  4661\n",
      "  Batch size =  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 583/583 [00:20<00:00, 27.79it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-48bbb59c5c2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-e0bc9a524fdc>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, tokenizer)\u001b[0m\n\u001b[1;32m     42\u001b[0m                                             tokenizer,)\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquad_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/data/metrics/squad_metrics.py\u001b[0m in \u001b[0;36msquad_evaluate\u001b[0;34m(examples, preds, no_answer_probs, no_answer_probability_threshold)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mno_answer_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mexact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_raw_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     exact_threshold = apply_no_ans_threshold(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/data/metrics/squad_metrics.py\u001b[0m in \u001b[0;36mget_raw_scores\u001b[0;34m(examples, preds)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mqas_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqas_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mgold_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manswers\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnormalize_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgold_answers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/data/metrics/squad_metrics.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mqas_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqas_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mgold_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manswers\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnormalize_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgold_answers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "results = evaluate(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in results.items():\n",
    "    print(\"eval_{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
