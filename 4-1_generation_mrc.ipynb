{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intense-deadline",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 4-1 Generation based MRC\n",
    "\n",
    "\n",
    "https://github.com/huggingface/transformers/tree/master/examples/research_projects/longform-qa 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aggressive-solution",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from transformers import BartConfig, BartTokenizer, BartForQuestionAnswering\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accredited-relief",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def make_qa_s2s_model(model_name=\"facebook/bart-large\", from_file=None, device=\"cuda:0\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device) # BartForConditionalGeneration\n",
    "    if from_file is not None:\n",
    "        param_dict = torch.load(from_file)  # has model weights, optimizer, and scheduler states\n",
    "        model.load_state_dict(param_dict[\"model\"])\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "similar-complex",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from transformers.data.metrics.squad_metrics import (\n",
    "    compute_predictions_logits,\n",
    "    squad_evaluate,\n",
    ")\n",
    "\n",
    "from transformers.data.processors.squad import SquadResult, SquadProcessor, squad_convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "former-locator",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import config as cfg\n",
    "from utils import load_and_cache_examples, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "quantitative-welding",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "from time import time\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import math \n",
    "from random import choice, randint\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import apex\n",
    "from apex import amp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-arbor",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-importance",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tribal-remains",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7)\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "squad = datasets.load_dataset(\"squad\")\n",
    "squad_train = squad[\"train\"]\n",
    "squad_valid = squad[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "further-input",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': {'answer_start': [565], 'text': ['1st overall']},\n",
       " 'context': 'In 2015-2016, Notre Dame ranked 18th overall among \"national universities\" in the United States in U.S. News & World Report\\'s Best Colleges 2016. In 2014, USA Today ranked Notre Dame 10th overall for American universities based on data from College Factual. Forbes.com\\'s America\\'s Best Colleges ranks Notre Dame 13th among colleges in the United States in 2015, 8th among Research Universities, and 1st in the Midwest. U.S. News & World Report also lists Notre Dame Law School as 22nd overall. BusinessWeek ranks Mendoza College of Business undergraduate school as 1st overall. It ranks the MBA program as 20th overall. The Philosophical Gourmet Report ranks Notre Dame\\'s graduate philosophy program as 15th nationally, while ARCHITECT Magazine ranked the undergraduate architecture program as 12th nationally. Additionally, the study abroad program ranks sixth in highest participation percentage in the nation, with 57.6% of students choosing to study abroad in 17 countries. According to payscale.com, undergraduate alumni of University of Notre Dame have a mid-career median salary $110,000, making it the 24th highest among colleges and universities in the United States. The median starting salary of $55,300 ranked 58th in the same peer group.',\n",
       " 'id': '5733afd3d058e614000b6048',\n",
       " 'question': 'The undergrad school at the Mendoza College of Business was ranked where according to BusinessWeek?',\n",
       " 'title': 'University_of_Notre_Dame'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train[46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "automotive-selection",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87599"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(squad_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "devoted-journalist",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# eli5 = datasets.load_dataset(\"eli5\", name=\"LFQA_reddit\")\n",
    "# eli5_train = eli5[\"train_eli5\"]\n",
    "# eli5_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-segment",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## load model, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "drawn-cosmetic",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tokenizer, model = make_qa_s2s_model(model_name = \"facebook/bart-large\")\n",
    "# \"Primer/bart-squad2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "imperial-genius",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class SquadDatasetS2S(Dataset):\n",
    "    def __init__(\n",
    "        self, examples_array, tokenizer\n",
    "    ):\n",
    "        self.data = examples_array\n",
    "        self.tokenizer = tokenizer\n",
    "        # 모든 데이터가 답이 하나 뿐이라 qa_id_list 필요없음\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) \n",
    "\n",
    "    def make_example(self, idx):\n",
    "        example = self.data[idx]\n",
    "        question = example[\"question\"]\n",
    "#         answer = example[\"answers\"][\"text\"][0] + \" \" + self.tokenizer.eos_token\n",
    "        answer = example[\"answers\"][\"text\"][0]\n",
    "        q_id = example[\"id\"]\n",
    "\n",
    "        document = example[\"context\"]\n",
    "        in_st = \"question: {} context: {}\".format(\n",
    "            question.lower().replace(\" --t--\", \"\").strip(), document.lower().strip(),\n",
    "        )\n",
    "        out_st = answer\n",
    "        return (in_st, out_st)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.make_example(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "academic-airline",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def make_qa_s2s_batch(qa_list, tokenizer, max_len=64, max_a_len=360, device=\"cuda:0\"):\n",
    "    q_ls = [q for q, a in qa_list]\n",
    "    a_ls = [a for q, a in qa_list]\n",
    "    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "    q_ids, q_mask = (\n",
    "        torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n",
    "        torch.LongTensor(q_toks[\"attention_mask\"]).to(device),\n",
    "    )\n",
    "    a_toks = tokenizer.batch_encode_plus(a_ls, max_length=min(max_len, max_a_len), padding=\"max_length\", truncation=True)\n",
    "    a_ids, a_mask = (\n",
    "        torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n",
    "        torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n",
    "    )\n",
    "    lm_labels = a_ids[:, 1:].contiguous().clone()\n",
    "    lm_labels[a_mask[:, 1:].contiguous() == 0] = -100\n",
    "    model_inputs = {\n",
    "        \"input_ids\": q_ids,\n",
    "        \"attention_mask\": q_mask,\n",
    "        \"decoder_input_ids\": a_ids[:, :-1].contiguous(),\n",
    "        \"labels\": lm_labels,\n",
    "    }\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "assisted-deviation",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# mode = 'train'\n",
    "# train_dataset = load_and_cache_examples(cfg, tokenizer, mode_or_filename=mode, output_examples=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "disciplinary-polymer",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_dset = SquadDatasetS2S(squad_train, tokenizer)\n",
    "valid_dset = SquadDatasetS2S(squad_valid, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dedicated-taiwan",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "len_tr = 10000\n",
    "len_vl = 2000\n",
    "train_dset = torch.utils.data.random_split(train_dset, [len_tr, len(train_dset) - len_tr])[0]\n",
    "valid_dset = torch.utils.data.random_split(valid_dset, [len_vl, len(valid_dset) - len_vl])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "saving-wiring",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('question: what story was written by child in 1842? context: the figure of the \"tragic octoroon\" was a stock character of abolitionist literature: a mixed-race woman raised as if a white woman in her white father\\'s household, until his bankruptcy or death has her reduced to a menial position she may even be unaware of her status before being reduced to victimization. the first character of this type was the heroine of lydia maria child\\'s \"the quadroons\" (1842), a short story. this character allowed abolitionists to draw attention to the sexual exploitation in slavery and, unlike portrayals of the suffering of the field hands, did not allow slaveholders to retort that the sufferings of northern mill hands were no easier. the northern mill owner would not sell his own children into slavery.',\n",
       " '\"The Quadroons\"')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-teaching",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "round-primary",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# from transformers import BartModel\n",
    "# test_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "# test_model = BartModel.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# device = \"cuda:0\"\n",
    "# qa_list = [train_dset[1], train_dset[2]]\n",
    "# q_ls = [q for q, a in qa_list]\n",
    "# a_ls = [a for q, a in qa_list]\n",
    "# q_toks = test_tokenizer.batch_encode_plus(q_ls, max_length=64, padding=\"max_length\", truncation=True)\n",
    "# q_ids, q_mask = (\n",
    "#     torch.LongTensor(q_toks[\"input_ids\"]),\n",
    "#     torch.LongTensor(q_toks[\"attention_mask\"]),\n",
    "# )\n",
    "\n",
    "# a_toks = test_tokenizer.batch_encode_plus(a_ls, max_length=min(64, 360), padding=\"max_length\", truncation=True)\n",
    "# a_ids, a_mask = (\n",
    "#     torch.LongTensor(a_toks[\"input_ids\"]),\n",
    "#     torch.LongTensor(a_toks[\"attention_mask\"]),\n",
    "# )\n",
    "\n",
    "# # q_ids.shape\n",
    "\n",
    "# # a_ids.shape # 0~64\n",
    "\n",
    "# lm_labels = a_ids[:, 1:].contiguous().clone()\n",
    "\n",
    "# # lm_labels.shape # 1번째부터 64번째까지\n",
    "\n",
    "# # lm_labels\n",
    "\n",
    "# lm_labels[a_mask[:, 1:].contiguous() == 0] = -100\n",
    "\n",
    "# # lm_labels\n",
    "\n",
    "# model_inputs = {\n",
    "#     \"input_ids\": q_ids,\n",
    "#     \"attention_mask\": q_mask,\n",
    "#     \"decoder_input_ids\": a_ids[:, :-1].contiguous(),\n",
    "#     \"return_dict\": True,\n",
    "#     #     \"labels\": lm_labels,\n",
    "# }\n",
    "\n",
    "# outputs = test_model(**model_inputs)\n",
    "\n",
    "# # 'last_hidden_state', 'past_key_values', 'decoder_hidden_states', \n",
    "# # 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state',\n",
    "# # 'encoder_hidden_states', 'encoder_attentions'\n",
    "\n",
    "# # 3개만 나올땐 last_hidden_state past_key_values encoder_last_hidden_state 이렇게 3개인듯\n",
    "\n",
    "# outputs.__dict__.keys()\n",
    "\n",
    "# print(outputs[0].shape) # 출력 모양, (bs, seq_len, hidden_dim)\n",
    "# print(test_model.shared.num_embeddings) # 단어 수 \n",
    "\n",
    "# import torch.nn as nn\n",
    "# lm_head = nn.Linear(outputs[0].shape[-1], test_model.shared.num_embeddings, bias=False)\n",
    "\n",
    "# lm_logits = lm_head(outputs[0])\n",
    "# print(lm_logits.shape) # 출력 시퀀스 내 각 포지션 별 로짓\n",
    "\n",
    "# print(lm_labels.shape) # (bs, max_seq_len - 1)\n",
    "# print(lm_labels.view(-1).shape) # (bs x max_seq_len -1)\n",
    "# print(lm_logits.view(-1, 50265).shape) # (bs x max_seq_len -1, vocab_size)\n",
    "\n",
    "# loss_fct = nn.CrossEntropyLoss()\n",
    "# masked_lm_loss = loss_fct(lm_logits.view(-1, 50265), lm_labels.view(-1))\n",
    "# print(masked_lm_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-strike",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-camping",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 함수꼴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "representative-minute",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def train_qa_s2s_epoch(model, dataset, tokenizer, optimizer, scheduler, args, e=0, curriculum=False):\n",
    "    model.train()\n",
    "    # make iterator\n",
    "    if curriculum:\n",
    "        train_sampler = SequentialSampler(dataset)\n",
    "    else:\n",
    "        train_sampler = RandomSampler(dataset)\n",
    "    model_collate_fn = functools.partial(\n",
    "        make_qa_s2s_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda:0\"\n",
    "    )\n",
    "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n",
    "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
    "    # accumulate loss since last print\n",
    "    loc_steps = 0\n",
    "    loc_loss = 0.0\n",
    "    st_time = time()\n",
    "    for step, batch_inputs in enumerate(epoch_iterator):\n",
    "        loss = model(**batch_inputs)[0]\n",
    "#         loss = pre_loss.sum() / pre_loss.shape[0] # 배치단위로 평균 내 줄 수 있으나, bartcondgen 은 필요없음\n",
    "        \n",
    "        # amp\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "\n",
    "        # optimizer\n",
    "        if step % args.backward_freq == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "        # some printing within the epoch\n",
    "        loc_loss += loss.item()\n",
    "        loc_steps += 1\n",
    "        if step % args.print_freq == 0 or step == 1:\n",
    "            print(\n",
    "                \"{:2d} {:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
    "                    e, step, len(dataset) // args.batch_size, loc_loss / loc_steps, time() - st_time,\n",
    "                )\n",
    "            )\n",
    "            loc_loss = 0\n",
    "            loc_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "exact-campus",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def eval_qa_s2s_epoch(model, dataset, tokenizer, args):\n",
    "    model.eval()\n",
    "    # make iterator\n",
    "    train_sampler = SequentialSampler(dataset)\n",
    "    model_collate_fn = functools.partial(\n",
    "        make_qa_s2s_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda:0\"\n",
    "    )\n",
    "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n",
    "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
    "    # accumulate loss since last print\n",
    "    loc_steps = 0\n",
    "    loc_loss = 0.0\n",
    "    st_time = time()\n",
    "    with torch.no_grad():\n",
    "        for step, batch_inputs in enumerate(epoch_iterator):\n",
    "            loss = model(**batch_inputs)[0]\n",
    "#             loss = pre_loss.sum() / pre_loss.shape[0]\n",
    "            loc_loss += loss.item()\n",
    "            loc_steps += 1\n",
    "            if step % args.print_freq == 0:\n",
    "                print(\n",
    "                    \"{:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
    "                        step,\n",
    "                        len(dataset) // args.batch_size,\n",
    "                        loc_loss / loc_steps,\n",
    "                        time() - st_time,\n",
    "                    )\n",
    "                )\n",
    "    print(\n",
    "        \"Total \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
    "            loc_loss / loc_steps,\n",
    "            time() - st_time,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cubic-raising",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def train_qa_s2s(qa_s2s_model, qa_s2s_tokenizer, s2s_train_dset, s2s_valid_dset, s2s_args):\n",
    "#     s2s_optimizer = AdamW(qa_s2s_model.parameters(), lr=s2s_args.learning_rate, eps=1e-8)\n",
    "    s2s_optimizer = apex.optimizers.FusedLAMB(qa_s2s_model.parameters(),\n",
    "                                    lr = s2s_args.learning_rate,\n",
    "                                    eps=1e-8,\n",
    "                                    weight_decay=0.0,\n",
    "                                    max_grad_norm=1.0)\n",
    "    qa_s2s_model, s2s_optimizer = amp.initialize(qa_s2s_model, s2s_optimizer, opt_level=\"O1\")\n",
    "    s2s_scheduler = get_linear_schedule_with_warmup(\n",
    "        s2s_optimizer,\n",
    "        num_warmup_steps=400,\n",
    "        num_training_steps=(s2s_args.num_epochs + 1) * math.ceil(len(s2s_train_dset) / s2s_args.batch_size),\n",
    "    )\n",
    "    for e in range(s2s_args.num_epochs):\n",
    "        train_qa_s2s_epoch(\n",
    "            qa_s2s_model,\n",
    "            s2s_train_dset,\n",
    "            qa_s2s_tokenizer,\n",
    "            s2s_optimizer,\n",
    "            s2s_scheduler,\n",
    "            s2s_args,\n",
    "            e,\n",
    "            curriculum=(e == 0),\n",
    "        )\n",
    "        m_save_dict = {\n",
    "            \"model\": qa_s2s_model.state_dict(),\n",
    "            \"optimizer\": s2s_optimizer.state_dict(),\n",
    "            \"scheduler\": s2s_scheduler.state_dict(),\n",
    "        }\n",
    "        print(\"Saving model {}\".format(s2s_args.model_save_name))\n",
    "        eval_qa_s2s_epoch(qa_s2s_model, s2s_valid_dset, qa_s2s_tokenizer, s2s_args)\n",
    "        torch.save(m_save_dict, \"{}_{}.pth\".format(s2s_args.model_save_name, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-pizza",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "front-somalia",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# training loop proper\n",
    "class ArgumentsS2S():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 4\n",
    "        self.backward_freq = 16\n",
    "        self.max_length = 512\n",
    "        self.print_freq = 100\n",
    "        self.model_save_name = \"seq2seq_models/squad_bart_model1\"\n",
    "        self.learning_rate = 1e-4\n",
    "        self.num_epochs = 2\n",
    "\n",
    "s2s_args = ArgumentsS2S()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-photography",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      " 0     0 of  2500 \t L: 9.341 \t -- 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0     1 of  2500 \t L: 11.416 \t -- 0.969\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0\n",
      " 0   100 of  2500 \t L: 9.972 \t -- 24.370\n",
      " 0   200 of  2500 \t L: 8.324 \t -- 48.321\n",
      " 0   300 of  2500 \t L: 5.689 \t -- 72.754\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0\n",
      " 0   400 of  2500 \t L: 4.499 \t -- 97.859\n",
      " 0   500 of  2500 \t L: 3.821 \t -- 122.450\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0\n",
      " 0   600 of  2500 \t L: 3.201 \t -- 147.500\n",
      " 0   700 of  2500 \t L: 2.778 \t -- 172.067\n",
      " 0   800 of  2500 \t L: 2.214 \t -- 196.247\n",
      " 0   900 of  2500 \t L: 1.943 \t -- 220.488\n",
      " 0  1000 of  2500 \t L: 1.635 \t -- 245.155\n",
      " 0  1100 of  2500 \t L: 1.539 \t -- 269.701\n",
      " 0  1200 of  2500 \t L: 1.387 \t -- 294.863\n",
      " 0  1300 of  2500 \t L: 1.409 \t -- 318.182\n",
      " 0  1400 of  2500 \t L: 1.165 \t -- 344.592\n"
     ]
    }
   ],
   "source": [
    "train_qa_s2s(model, tokenizer, train_dset, valid_dset, s2s_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-arctic",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "\n",
    "# generate answer from input \"question: ... context: <p> ...\"\n",
    "def qa_s2s_generate(\n",
    "    question_doc,\n",
    "    qa_s2s_model,\n",
    "    qa_s2s_tokenizer,\n",
    "    num_answers=1,\n",
    "    num_beams=None,\n",
    "    min_len=64,\n",
    "    max_len=256,\n",
    "    do_sample=False,\n",
    "    temp=1.0,\n",
    "    top_p=None,\n",
    "    top_k=None,\n",
    "    max_input_length=512,\n",
    "    device=\"cuda:0\",\n",
    "):\n",
    "    model_inputs = make_qa_s2s_batch(\n",
    "        [(question_doc, \"A\")],\n",
    "        qa_s2s_tokenizer,\n",
    "        max_input_length,\n",
    "        device=device,\n",
    "    )\n",
    "    n_beams = num_answers if num_beams is None else max(num_beams, num_answers)\n",
    "    generated_ids = qa_s2s_model.generate(\n",
    "        input_ids=model_inputs[\"input_ids\"],\n",
    "        attention_mask=model_inputs[\"attention_mask\"],\n",
    "        min_length=min_len,\n",
    "        max_length=max_len,\n",
    "        do_sample=do_sample,\n",
    "        early_stopping=True,\n",
    "        num_beams=1 if do_sample else n_beams,\n",
    "        temperature=temp,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        eos_token_id=qa_s2s_tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_return_sequences=num_answers,\n",
    "        decoder_start_token_id=qa_s2s_tokenizer.bos_token_id,\n",
    "    )\n",
    "    return [qa_s2s_tokenizer.decode(ans_ids, skip_special_tokens=True).strip() for ans_ids in generated_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-state",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# tokenizer, model = make_qa_s2s_model(from_file=\"seq2seq_models/squad_bart_model_.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-monte",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "\n",
    "example = squad_valid[1634]\n",
    "\n",
    "print(f\"question = {example['question']}\")\n",
    "print(f\"original answer = {example['answers']['text'][0]}\")\n",
    "question_document = \"question: {} context: {}\".format(example['question'], example['context'])\n",
    "answer = qa_s2s_generate(question_document, model, tokenizer,\n",
    "                         max_len = 20, top_p=0.95, top_k=30,\n",
    "                         device=\"cuda:0\"\n",
    "                        )\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"generated answer = {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-lender",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pri\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-moral",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-aurora",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-ceiling",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-documentary",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
