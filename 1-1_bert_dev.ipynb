{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT dev\n",
    "\n",
    "작성일자: 210116 \\\n",
    "작성자: 조진욱\\\n",
    "목표: HF 기반 BERT 가 어떻게 돌아가는지 알아보자\\\n",
    "비고: \n",
    "1. 학습이 안되어있는 linear layer 을 사용하므로 성능은 안좋을것임\n",
    "2. 원래 util 에 있는 함수들을 눈에 보이도록 코드상에 두었음. 앞으론 util 에서 불러올 예정\n",
    "\n",
    "\n",
    "레퍼런스 코드\n",
    "https://github.com/huggingface/transformers/blob/master/examples/legacy/question-answering/run_squad.py\n",
    "\n",
    "가급적이면 수정사항을 적어두려고함(TO DO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    BertForQuestionAnswering,\n",
    "    BertTokenizer,\n",
    ")\n",
    "from transformers.data.metrics.squad_metrics import (\n",
    "    compute_predictions_logits,\n",
    "    squad_evaluate,\n",
    ")\n",
    "\n",
    "from transformers.data.processors.squad import SquadResult, SquadProcessor, squad_convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadV1Processor(SquadProcessor):\n",
    "    train_file = \"train.json\"\n",
    "    dev_file = \"dev.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, tokenizer, mode_or_filename, output_examples=False):\n",
    "    \"\"\"\n",
    "    Changes\n",
    "        1. no distributed training(removed for simplicity)\n",
    "        2. no caching(cache make preprocessing time shorter, but removed for simplicity)\n",
    "    \"\"\"\n",
    "    input_dir = args.squad_dir if args.squad_dir else args.data_dir\n",
    "\n",
    "    print(\"Creating features from dataset file at %s\", input_dir)\n",
    "\n",
    "    if mode_or_filename == \"train\" or mode_or_filename == \"dev\" or mode_or_filename == \"test\":\n",
    "        mode = mode_or_filename\n",
    "        processor = SquadV1Processor()\n",
    "        if mode == 'test':\n",
    "            examples = processor.get_dev_examples(args.squad_dir, filename=processor.test_file)\n",
    "        elif mode == 'dev':\n",
    "            examples = processor.get_dev_examples(args.squad_dir, filename=processor.dev_file)\n",
    "        else:\n",
    "            examples = processor.get_train_examples(args.squad_dir, filename=processor.train_file)\n",
    "    else:\n",
    "        # odqa 에 사용되는 데이터들을 처리 하기 위한 용도. 여기선 필요없으므로 삭제\n",
    "        pass\n",
    "\n",
    "    features, dataset = squad_convert_examples_to_features(\n",
    "        examples=examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=args.max_seq_length,\n",
    "        doc_stride=args.doc_stride,\n",
    "        max_query_length=args.max_query_length,\n",
    "        is_training=True if mode == 'train' else False,\n",
    "        return_dataset='pt',\n",
    "        threads=args.threads,\n",
    "    )\n",
    "\n",
    "#     torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n",
    "\n",
    "    if output_examples:\n",
    "        return dataset, examples, features\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(cfg.model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(cfg.tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features from dataset file at %s ./data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 17.34it/s]\n",
      "convert squad examples to features: 100%|██████████| 4639/4639 [00:06<00:00, 696.98it/s]\n",
      "add example index and unique id: 100%|██████████| 4639/4639 [00:00<00:00, 1341425.46it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset, examples, features = load_and_cache_examples(cfg, tokenizer, mode='dev', output_examples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_sampler = SequentialSampler(dataset)\n",
    "eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=cfg.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 582/582 [00:59<00:00,  9.71it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(cfg.device) for t in batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"token_type_ids\": batch[2],\n",
    "        }\n",
    "\n",
    "        feature_indices = batch[3]\n",
    "#         print(feature_indices)\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    for i, feature_index in enumerate(feature_indices):\n",
    "        eval_feature = features[feature_index.item()]\n",
    "        unique_id = int(eval_feature.unique_id)\n",
    "\n",
    "        start_logits = outputs.start_logits[i]\n",
    "        end_logits = outputs.end_logits[i]\n",
    "        result = SquadResult(unique_id, start_logits, end_logits)\n",
    "\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_instance_attributes(obj):\n",
    "    for attribute, value in obj.__dict__.items():\n",
    "        print(attribute, '=', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = None\n",
      "start_logits = tensor([[-0.1530,  0.3589,  0.0857,  ...,  0.3976,  0.5834,  0.5727],\n",
      "        [-0.1354,  0.4048,  0.2308,  ...,  0.4453,  0.6002,  0.5315],\n",
      "        [-0.0866,  0.2840,  0.4452,  ...,  0.6871,  0.5968, -0.0071],\n",
      "        [-0.0618,  0.2369, -0.9219,  ...,  0.6738,  0.8392,  0.9667],\n",
      "        [-0.1048,  0.2697,  0.0457,  ...,  0.0706,  0.4150,  0.3221],\n",
      "        [-0.1097,  0.1649, -0.9500,  ...,  0.8741,  0.2284,  0.4844]],\n",
      "       device='cuda:0')\n",
      "end_logits = tensor([[ 0.1823,  0.3414,  0.1204,  ..., -0.0666, -0.2827, -0.1326],\n",
      "        [ 0.1997,  0.3687,  0.0925,  ..., -0.0860, -0.2994, -0.0506],\n",
      "        [ 0.2192,  0.2996, -0.4997,  ..., -0.2378, -0.3344, -0.7197],\n",
      "        [ 0.2411,  0.3736,  0.0279,  ..., -0.4288, -0.2715, -0.3000],\n",
      "        [ 0.1903,  0.3309,  0.4197,  ..., -0.4447, -0.4841, -0.4865],\n",
      "        [ 0.2125,  0.3412, -0.5083,  ...,  0.3277, -0.1746,  0.0188]],\n",
      "       device='cuda:0')\n",
      "hidden_states = None\n",
      "attentions = None\n"
     ]
    }
   ],
   "source": [
    "print_instance_attributes(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 384])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.start_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = compute_predictions_logits(\n",
    "    examples,\n",
    "    features,\n",
    "    all_results,\n",
    "    cfg.n_best_size,\n",
    "    cfg.max_answer_length,\n",
    "    True,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    cfg.verbose_logging,\n",
    "    False,\n",
    "    cfg.null_score_diff_threshold,\n",
    "    tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: OrderedDict([('exact', 0.04311273981461522), ('f1', 4.963176493011408), ('total', 4639), ('HasAns_exact', 0.04311273981461522), ('HasAns_f1', 4.963176493011408), ('HasAns_total', 4639), ('best_exact', 0.04311273981461522), ('best_exact_thresh', 0.0), ('best_f1', 4.963176493011408), ('best_f1_thresh', 0.0)])\n"
     ]
    }
   ],
   "source": [
    "# Compute the F1 and exact scores.\n",
    "results = squad_evaluate(examples, predictions)\n",
    "print(\"Results: {}\".format(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}