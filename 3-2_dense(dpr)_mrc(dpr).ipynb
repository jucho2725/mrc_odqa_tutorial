{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    BertForQuestionAnswering,\n",
    "    BertTokenizer,\n",
    ")\n",
    "from transformers.data.metrics.squad_metrics import (\n",
    "    compute_predictions_logits,\n",
    "    squad_evaluate,\n",
    ")\n",
    "\n",
    "from transformers.data.processors.squad import SquadResult, SquadProcessor, squad_convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadV1Processor(SquadProcessor):\n",
    "    train_file = \"train.json\"\n",
    "    dev_file = \"dev.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, tokenizer, mode, evaluate=False, output_examples=False):\n",
    "    \"\"\"\n",
    "    Reference: https://github.com/huggingface/transformers/blob/master/examples/legacy/question-answering/run_squad.py\n",
    "    \n",
    "    Changes \n",
    "        1. no distributed training(removed for simplicity)\n",
    "        2. no caching(cache make preprocessing time shorter, but removed for simplicity)\n",
    "    \n",
    "    \"\"\"\n",
    "    input_dir = args.data_dir if args.data_dir else \".\"\n",
    "\n",
    "    print(\"Creating features from dataset file at %s\", input_dir)\n",
    "    processor = SquadV1Processor()\n",
    "    if mode == 'test':\n",
    "        examples = processor.get_dev_examples(args.data_dir, filename=processor.test_file)\n",
    "    elif mode == 'dev':\n",
    "        examples = processor.get_dev_examples(args.data_dir, filename=processor.dev_file)\n",
    "    else:\n",
    "        examples = processor.get_train_examples(args.data_dir, filename=processor.train_file)\n",
    "\n",
    "    features, dataset = squad_convert_examples_to_features(\n",
    "        examples=examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=args.max_seq_length,\n",
    "        doc_stride=args.doc_stride,\n",
    "        max_query_length=args.max_query_length,\n",
    "        is_training=True if mode == 'train' else False,\n",
    "        return_dataset='pt',\n",
    "        threads=args.threads,\n",
    "    )\n",
    "\n",
    "#     torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n",
    "\n",
    "    if output_examples:\n",
    "        return dataset, examples, features\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# BERT + 마지막 cls 추가 레이어 존재함\n",
    "# 추가 레이어는 학습이 되어있지 않으므로, 아래 Some weights of the model checkpoint at bert-large-cased were not used 와 같은 에러 발생\n",
    "# 추후 과제로 낼 시 이 부분을 각자 customize 하도록 과제를 내도 좋을듯 함\n",
    "model = BertForQuestionAnswering.from_pretrained(cfg.model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(cfg.tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features from dataset file at %s ./data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:09<00:00, 21.59it/s]\n",
      "convert squad examples to features:  35%|███▍      | 13377/38708 [00:16<00:30, 819.10it/s]"
     ]
    }
   ],
   "source": [
    "train_dataset = load_and_cache_examples(cfg, tokenizer, mode=mode, output_examples=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=cfg.train_batch_size)\n",
    "\n",
    "t_total = len(train_dataloader) // cfg.gradient_accumulation_steps * cfg.num_train_epochs\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": cfg.weight_decay,\n",
    "    },\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=cfg.learning_rate, eps=cfg.adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=cfg.warmup_steps, num_training_steps=t_total\n",
    ")\n",
    "\n",
    "# Train!\n",
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\", len(train_dataset))\n",
    "print(\"  Num Epochs = %d\", cfg.num_train_epochs)\n",
    "print(\n",
    "    \"  Total train batch size = %d\",\n",
    "    cfg.train_batch_size\n",
    "    * cfg.gradient_accumulation_steps\n",
    ")\n",
    "print(\"  Gradient Accumulation steps = %d\", cfg.gradient_accumulation_steps)\n",
    "print(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "global_step = 1\n",
    "tr_loss = 0.0\n",
    "best_metrics = {'f1': 0, 'exact': 0, 'epoch': -1}\n",
    "model.zero_grad()\n",
    "# Added here for reproductibility\n",
    "set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for now_epoch in trange(int(cfg.num_train_epochs), desc=\"Epoch\"):\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "        model.train()\n",
    "        batch = tuple(t.to(cfg.device) for t in batch)\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"token_type_ids\": batch[2],\n",
    "            \"start_positions\": batch[3],\n",
    "            \"end_positions\": batch[4],\n",
    "            \"device\": cfg.device,\n",
    "        }\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if cfg.gradient_accumulation_steps > 1:\n",
    "            loss = loss / cfg.gradient_accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        if (step + 1) % cfg.gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "    results = evaluate(cfg, model, tokenizer, 'dev')\n",
    "\n",
    "    if best_metrics['f1'] < results['f1']:\n",
    "        best_metrics['f1'] = results['f1']\n",
    "        best_metrics['exact'] = results['exact']\n",
    "        best_metrics['epoch'] = now_epoch\n",
    "        model.save_pretrained(cfg.output_dir)\n",
    "\n",
    "    for key, value in results.items():\n",
    "        print(\"dev eval_{}: {}\".format(key, value))\n",
    "\n",
    "    for key, value in best_metrics.items():\n",
    "        print(\"dev best eval_{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(cfg.device) for t in batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"token_type_ids\": batch[2],\n",
    "        }\n",
    "\n",
    "        feature_indices = batch[3]\n",
    "#         print(feature_indices)\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    for i, feature_index in enumerate(feature_indices):\n",
    "        eval_feature = features[feature_index.item()]\n",
    "        unique_id = int(eval_feature.unique_id)\n",
    "\n",
    "        start_logits = outputs.start_logits[i]\n",
    "        end_logits = outputs.end_logits[i]\n",
    "        result = SquadResult(unique_id, start_logits, end_logits)\n",
    "\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_instance_attributes(obj):\n",
    "    for attribute, value in obj.__dict__.items():\n",
    "        print(attribute, '=', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_instance_attributes(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.start_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prediction_file = \"predictions_{}.json\".format(mode)\n",
    "output_nbest_file = \"nbest_predictions_{}.json\".format(mode)\n",
    "output_null_log_odds_file = \"null_log_odds_predictions_{}.json\".format(mode)\n",
    "predictions = compute_predictions_logits(\n",
    "    examples,\n",
    "    features,\n",
    "    all_results,\n",
    "    cfg.n_best_size,\n",
    "    cfg.max_answer_length,\n",
    "    True,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    cfg.verbose_logging,\n",
    "    False,\n",
    "    cfg.null_score_diff_threshold,\n",
    "    tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the F1 and exact scores.\n",
    "results = squad_evaluate(examples, predictions)\n",
    "print(\"Results: {}\".format(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
