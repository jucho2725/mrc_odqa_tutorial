{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF retrieval + mrc(pretrained)\n",
    "\n",
    "작성일자: 210118\\\n",
    "작성자: 조진욱\\\n",
    "목표: retrieval 모델과 우리가 학습시킨 bert 모델을 가지고 open domain qa 형식으로 만들어보자\\\n",
    "순서: \n",
    "1. \n",
    "query 가 나열되어있는 json 형태 파일 'dev_qa.json' 파일이 들어오면\n",
    "retrieval 모델이 query에 맞는 context 하나를 찾아 (c, q, a) pair 를 만들어줌. \n",
    "이를 dev_cqa.json 로 저장함.\n",
    "2. \n",
    "bert 모델은 dev_cqa.json 을 불러와 answer에 대한 답을 냄.\n",
    "그 뒤 squad_evaluate 함수를 통해 점수 확인 \n",
    "\n",
    "\n",
    "비고:\n",
    "1. load 함수에서 json 형태로 불러오도록 되어있어 필요없지만 retrieval 의 결과를 json 으로 저장하는 과정을 거침. \n",
    "2. 1-2 에서 학습한 bert 모델을 그대로 가져다씀 from_pretrained(cfg.output_dir)\n",
    "3. 그러나 1-2 best metric 보다 성능이 안나와야 정상. 왜냐하면 retrieval 과정에서 잘못된 context 들이 선택되었을 것이기 때문."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from scipy import spatial\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from utils import read_file, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retrieval(object):\n",
    "    def __init__(self, mode, data_path=None):\n",
    "        if not data_path:\n",
    "            self.data_path = './data/'\n",
    "        else:\n",
    "            self.data_path = data_path\n",
    "        self.mode = mode\n",
    "\n",
    "    def make_embedding(self, context_name):\n",
    "        # Pickle save.\n",
    "        pickle_name = \"sparse\"+ \"_\" + self.mode + \"_embedding.bin\"\n",
    "        pickle_path = os.path.join(self.data_path + pickle_name)\n",
    "\n",
    "        context_dict = read_file(self.data_path + context_name)\n",
    "        context = context_dict['text']\n",
    "\n",
    "        tfidfv = TfidfVectorizer(tokenizer=self.tokenize, ngram_range=(1,2)).fit(context)\n",
    "\n",
    "        if os.path.isfile(pickle_path):\n",
    "            with open(pickle_path, \"rb\") as file:\n",
    "                context_embeddings = pickle.load(file)\n",
    "            print(\"Embedding pickle load.\")\n",
    "        else:\n",
    "            context_embeddings = tfidfv.transform(context).toarray()\n",
    "            # Pickle save.\n",
    "            with open(pickle_path, \"wb\") as file:\n",
    "                pickle.dump(context_embeddings, file)\n",
    "            print(\"Embedding pickle saved.\")\n",
    "        return tfidfv, context_embeddings\n",
    "\n",
    "\n",
    "    def retrieval(self, model, sparse_embedding, file_name, topk=1):\n",
    "        # load qas file.\n",
    "        qas = json.load(open(self.data_path + file_name))['data']\n",
    "        \n",
    "        context_name = self.mode + \"_context.json\"\n",
    "        context_dict = read_file(self.data_path + context_name)\n",
    "        context = context_dict['text']\n",
    "        context_id = context_dict['ids']\n",
    "\n",
    "        cqas = pd.DataFrame()\n",
    "        que = []\n",
    "        que_id = []\n",
    "        con_id = []\n",
    "        con = []\n",
    "\n",
    "        for item in tqdm(qas):\n",
    "            query = item['question']\n",
    "            query_id = item['id']\n",
    "\n",
    "            query_s_embedding = model.transform([query]).toarray()\n",
    "            predict_dict = self.sparse_searching(query_s_embedding,\n",
    "                                                 sparse_embedding,\n",
    "                                                 context,\n",
    "                                                 topk)\n",
    "            que.append(query)\n",
    "            que_id.append(query_id)\n",
    "        \n",
    "            for i in range(len(predict_dict['text'])):\n",
    "                temp_context = predict_dict['text'][i]\n",
    "                con.append(temp_context)\n",
    "                con_id.append(context_id[predict_dict['ids'][i]])\n",
    "\n",
    "        cqas['question'] = que\n",
    "        cqas['q_id'] = que_id\n",
    "        cqas['context'] = con\n",
    "        cqas['context_id'] = con_id\n",
    "\n",
    "        return cqas\n",
    "\n",
    "    def sparse_searching(self, sparse_query, sparse_embedding, texts, topk=1):\n",
    "        distances = spatial.distance.cdist(sparse_query, sparse_embedding, \"cosine\")[0]\n",
    "        result = zip(range(len(distances)), distances)\n",
    "        result = sorted(result, key=lambda x: x[1])\n",
    "        \n",
    "        cand_dict = {}\n",
    "        candidate = []\n",
    "        cand_ids = []\n",
    "        for idx, distances in result[0:topk]: # top k \n",
    "            candidate.append(texts[idx])\n",
    "            cand_ids.append(idx)\n",
    "\n",
    "        cand_dict['text'] = candidate\n",
    "        cand_dict['ids'] = cand_ids\n",
    "        return cand_dict\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [word for word in word_tokenize(text)]\n",
    "        stems = [stemmer.stem(item) for item in tokens]\n",
    "        return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_file = f'{mode}_context.json'\n",
    "qa_file =  f'{mode}_qa.json'\n",
    "ret = Retrieval(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding pickle load.\n"
     ]
    }
   ],
   "source": [
    "# knowledge base에 있는 articles(context) 들의 정보를 임베딩해둠\n",
    "tfidfv, context_embeddings = ret.make_embedding(context_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65acd1bbc6da41dba98faed68ee53a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cqa_df = ret.retrieval(tfidfv, context_embeddings, qas_file)\n",
    "cqa_df.to_csv(f\"retrived_{mode}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(cqd_df, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    BertForQuestionAnswering,\n",
    "    BertTokenizer,\n",
    ")\n",
    "from transformers.data.metrics.squad_metrics import (\n",
    "    compute_predictions_logits,\n",
    "    squad_evaluate,\n",
    ")\n",
    "\n",
    "from transformers.data.processors.squad import SquadResult, SquadProcessor, squad_convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# BERT + 마지막 cls 추가 레이어 존재함\n",
    "# 이미 학습된 모델이므로  Some weights of the model checkpoint at bert-large-cased were not used 와 같은 에러 발생하면 안됨\n",
    "model = BertForQuestionAnswering.from_pretrained(cfg.output_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(cfg.tokenizer_name)\n",
    "model = model.to(cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:00<00:02,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features from dataset file at %s ./data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 18.75it/s]\n",
      "convert squad examples to features: 100%|██████████| 4639/4639 [00:06<00:00, 693.77it/s]\n",
      "add example index and unique id: 100%|██████████| 4639/4639 [00:00<00:00, 1421180.06it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset, examples, features = load_and_cache_examples(cfg, tokenizer, mode=mode, output_examples=True)\n",
    "eval_sampler = SequentialSampler(dataset)\n",
    "eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=cfg.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer):\n",
    "    print(\"***** Running evaluation *****\")\n",
    "    print(\"  Num examples = \", len(dataset))\n",
    "    print(\"  Batch size = \", cfg.eval_batch_size)\n",
    "    all_results = []\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(cfg.device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "            }\n",
    "\n",
    "            feature_indices = batch[3]\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        for i, feature_index in enumerate(feature_indices):\n",
    "            eval_feature = features[feature_index.item()]\n",
    "            unique_id = int(eval_feature.unique_id)\n",
    "\n",
    "            start_logits = outputs.start_logits[i]\n",
    "            end_logits = outputs.end_logits[i]\n",
    "            result = SquadResult(unique_id, start_logits, end_logits)\n",
    "\n",
    "            all_results.append(result)\n",
    "            \n",
    "    predictions = compute_predictions_logits(examples,\n",
    "                                            features,\n",
    "                                            all_results,\n",
    "                                            cfg.n_best_size,\n",
    "                                            cfg.max_answer_length,\n",
    "                                            True,\n",
    "                                            None,\n",
    "                                            None,\n",
    "                                            None,\n",
    "                                            cfg.verbose_logging,\n",
    "                                            False,\n",
    "                                            cfg.null_score_diff_threshold,\n",
    "                                            tokenizer,)\n",
    "    results = squad_evaluate(examples, predictions)\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(model, tokenizer)\n",
    "\n",
    "for key, value in results.items():\n",
    "    print(f\"{mode} eval_{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
